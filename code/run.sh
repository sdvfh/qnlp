#!/bin/bash
export TOKENIZERS_PARALLELISM=false

autogenerated_datasets=(chatgpt_easy chatgpt_medium chatgpt_hard)

quantum_classifiers=(singlerotx singleroty singlerotz rot rotcnot \
                     maouaki1 maouaki6 maouaki7 maouakiquasi7 \
                     maouaki9 maouaki15 ent1 ent2 ent3 ent4)
classical_classifiers=(svmrbf svmlinear svmpoly logistic randomforest knn mlp)

for dataset in "${autogenerated_datasets[@]}"; do
  for classifier in "${quantum_classifiers[@]}"; do
    for layers in 1 10; do
      python experiments.py -dataset "$dataset" -model_transformer "all-mpnet-base-v2"                   -n_features 768 --n_qubits 10 -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "all-mpnet-base-v2"                   -n_features 32  --n_qubits 5  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "all-mpnet-base-v2"                   -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "nomic-ai/nomic-embed-text-v1.5"      -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "tomaarsen/mpnet-base-nli-matryoshka" -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ;
    done
  done
done

for dataset in "${autogenerated_datasets[@]}"; do
  for classifier in "${classical_classifiers[@]}"; do
    for layers in 1; do
      python experiments.py -dataset "$dataset" -model_transformer "all-mpnet-base-v2"                   -n_features 768 --n_qubits 10 -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "all-mpnet-base-v2"                   -n_features 32  --n_qubits 5  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "all-mpnet-base-v2"                   -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "nomic-ai/nomic-embed-text-v1.5"      -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ; \
      python experiments.py -dataset "$dataset" -model_transformer "tomaarsen/mpnet-base-nli-matryoshka" -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 25 --n_repetitions 30 --n_layers "$layers" ;
    done
  done
done

for dataset in sst; do
  for classifier in "${quantum_classifiers[@]}"; do
    for layers in 1 10; do
      python experiments.py -dataset "$dataset" -model_transformer "tomaarsen/mpnet-base-nli-matryoshka" -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 512 --n_repetitions 30 --n_layers "$layers" ;
    done
  done
done

for dataset in sst; do
  for classifier in "${classical_classifiers[@]}"; do
    for layers in 1; do
      python experiments.py -dataset "$dataset" -model_transformer "tomaarsen/mpnet-base-nli-matryoshka" -n_features 16  --n_qubits 4  -model_classifier "$classifier" --epochs 30 --batch_size 512 --n_repetitions 30 --n_layers "$layers" ;
    done
  done
done
